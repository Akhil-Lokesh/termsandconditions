# Baseline Corpus Collection Guide

## ðŸ“š Overview

The baseline corpus consists of **100+ standard Terms & Conditions** documents used for anomaly detection. This guide explains how to collect, validate, and index the corpus.

---

## ðŸŽ¯ Collection Goals

| Metric | Target | Purpose |
|--------|--------|---------|
| **Total Documents** | 100+ | Statistical significance |
| **Categories** | 5 | Industry diversity |
| **Docs per Category** | 20+ | Balanced representation |
| **Minimum Pages** | 2 | Sufficient content |
| **Minimum Length** | 500 chars | Extractable text |

---

## ðŸ“ Corpus Structure

```
data/baseline_corpus/
â”œâ”€â”€ tech/                    # Technology companies (25+ docs)
â”‚   â”œâ”€â”€ google_tos.pdf
â”‚   â”œâ”€â”€ facebook_tos.pdf
â”‚   â””â”€â”€ ...
â”œâ”€â”€ ecommerce/               # E-commerce platforms (25+ docs)
â”‚   â”œâ”€â”€ amazon_tos.pdf
â”‚   â”œâ”€â”€ ebay_tos.pdf
â”‚   â””â”€â”€ ...
â”œâ”€â”€ saas/                    # SaaS companies (20+ docs)
â”‚   â”œâ”€â”€ slack_tos.pdf
â”‚   â”œâ”€â”€ notion_tos.pdf
â”‚   â””â”€â”€ ...
â”œâ”€â”€ finance/                 # Financial services (15+ docs)
â”‚   â”œâ”€â”€ paypal_tos.pdf
â”‚   â”œâ”€â”€ stripe_tos.pdf
â”‚   â””â”€â”€ ...
â”œâ”€â”€ general/                 # General services (25+ docs)
â”‚   â”œâ”€â”€ uber_tos.pdf
â”‚   â”œâ”€â”€ airbnb_tos.pdf
â”‚   â””â”€â”€ ...
â”œâ”€â”€ metadata.json            # Document metadata
â””â”€â”€ indexing_results.json    # Indexing statistics
```

---

## ðŸš€ Quick Start

### Option 1: Automated Collection (Recommended)

```bash
# 1. Install dependencies
cd backend
pip install playwright httpx beautifulsoup4
playwright install chromium

# 2. Run collection script
python scripts/collect_baseline_corpus.py --output data/baseline_corpus

# 3. Validate collected documents
python scripts/validate_corpus.py

# 4. Index to Pinecone
python scripts/index_baseline_corpus.py

# 5. Analyze statistics
python scripts/analyze_corpus_stats.py --check-index --visualize
```

### Option 2: Manual Collection

See [Manual Collection Process](#manual-collection-process) below.

---

## ðŸ¤– Automated Collection

### Prerequisites

```bash
# Install required packages
pip install playwright httpx beautifulsoup4

# Install Chromium browser
playwright install chromium
```

### Collection Script Features

- âœ… **95+ pre-configured sources** across 5 categories
- âœ… **Automatic web-to-PDF conversion** using Playwright
- âœ… **Direct PDF download** support
- âœ… **Resume capability** (skips existing files)
- âœ… **Rate limiting** (respectful to servers)
- âœ… **Progress tracking** and metadata generation
- âœ… **Error recovery** with detailed logging

### Usage Examples

```bash
# Collect all categories
python scripts/collect_baseline_corpus.py

# Collect specific categories
python scripts/collect_baseline_corpus.py --category tech saas

# Force re-download existing files
python scripts/collect_baseline_corpus.py --force

# Faster collection (use with caution)
python scripts/collect_baseline_corpus.py --delay 0.5
```

### Expected Output

```
ðŸ“š Starting collection of 95 documents across 5 categories
============================================================
ðŸ“ Category: TECH (25 sources)
============================================================

[1/25] Processing: Google
   Converting to PDF: https://policies.google.com/terms
   âœ“ Converted: google_tos.pdf (156KB)

[2/25] Processing: Facebook
   Converting to PDF: https://www.facebook.com/legal/terms
   âœ“ Converted: facebook_tos.pdf (89KB)

...

ðŸ“Š COLLECTION SUMMARY
============================================================
âœ“ Successful:     87
âŠ™ Already Exists: 5
âœ— Failed:         3
ðŸ“ Total:         95

ðŸ’¾ Metadata saved to: data/baseline_corpus/metadata.json
```

### Troubleshooting Collection

| Issue | Cause | Solution |
|-------|-------|----------|
| Playwright not found | Not installed | `playwright install chromium` |
| Timeout errors | Slow server | Increase timeout: edit script, change `timeout=30000` |
| Rate limiting (429) | Too fast | Increase delay: `--delay 3.0` |
| Empty PDFs | Page not loaded | Some sites require JavaScript - may need manual collection |
| SSL errors | Certificate issues | Update certificates or use manual collection |

---

## ðŸ“ Manual Collection Process

For sites that don't work with automated collection:

### Step-by-Step

1. **Visit company website**
   - Navigate to footer or legal section
   - Find "Terms of Service" or "Terms & Conditions"

2. **Save as PDF**
   - **Chrome**: Print â†’ Save as PDF
   - **Firefox**: Print â†’ Save to PDF
   - **Safari**: File â†’ Export as PDF

3. **Name consistently**
   ```
   {company_name}_tos.pdf
   ```
   Examples:
   - `google_tos.pdf`
   - `amazon_tos.pdf`
   - `slack_tos.pdf`

4. **Place in category folder**
   ```
   data/baseline_corpus/tech/google_tos.pdf
   data/baseline_corpus/ecommerce/amazon_tos.pdf
   data/baseline_corpus/saas/slack_tos.pdf
   ```

5. **Update metadata.json**
   ```json
   {
     "filename": "google_tos.pdf",
     "category": "tech",
     "company": "Google",
     "source_url": "https://policies.google.com/terms",
     "collected_at": "2025-01-15T10:30:00",
     "status": "manual"
   }
   ```

---

## âœ… Validation

### Automatic Validation

```bash
# Validate entire corpus
python scripts/validate_corpus.py

# Validate specific categories
python scripts/validate_corpus.py --category tech saas

# Generate detailed report
python scripts/validate_corpus.py --detailed --output validation_report.json
```

### Validation Checks

The validator performs these checks:

1. âœ… **File Readability** - Can the PDF be opened?
2. âœ… **Text Extraction** - Can text be extracted?
3. âœ… **Minimum Pages** - At least 2 pages
4. âœ… **Minimum Content** - At least 500 characters
5. âœ… **Duplicate Detection** - Content hash comparison
6. âœ… **Language Check** - Appears to be English
7. âœ… **Metadata Completeness** - All required fields present

### Expected Output

```
ðŸ” CORPUS VALIDATION
============================================================
Found 95 PDF files

[1/95] Validating: google_tos.pdf
   âœ“ Valid - 12 pages, 45,234 chars

[2/95] Validating: facebook_tos.pdf
   âœ“ Valid - 8 pages, 32,109 chars

...

âš ï¸  Found 2 sets of duplicate documents:
   â€¢ amazon_tos.pdf, amazon_marketplace_tos.pdf

ðŸ“Š VALIDATION SUMMARY
============================================================
Total Files:    95
âœ“ Valid:        92
âœ— Invalid:      3
âš ï¸  Warnings:    2
âŒ Errors:       3
ðŸ”„ Duplicates:  2

ðŸ“ˆ STATISTICS:
Total Pages:    852
Total Chars:    3,456,789
Avg Pages/Doc:  8.9
Avg Chars/Doc:  36,387
Page Range:     2 - 45

âœ… CORPUS VALIDATION PASSED
   (2 warnings)
```

### Fixing Validation Issues

| Issue | Action |
|-------|--------|
| **Text too short** | Re-save PDF with better quality, or remove if scanned |
| **Extraction failed** | Try different PDF tool or manual text extraction |
| **Duplicates found** | Remove duplicate or keep if from different jurisdiction |
| **Missing metadata** | Add to `metadata.json` manually |

---

## ðŸ—‚ï¸ Indexing to Pinecone

### Prerequisites

- Baseline corpus collected and validated
- Pinecone API key configured in `.env`
- OpenAI API key configured in `.env`

### Indexing Process

```bash
# Index all documents to Pinecone baseline namespace
python scripts/index_baseline_corpus.py

# Dry run (test without uploading)
python scripts/index_baseline_corpus.py --dry-run

# Force re-index
python scripts/index_baseline_corpus.py --force

# Index specific categories
python scripts/index_baseline_corpus.py --category tech saas
```

### What Happens During Indexing

For each document:

1. **Extract text** from PDF (PyPDF2/pdfplumber)
2. **Parse structure** into sections and clauses
3. **Create chunks** (500 words, 50 word overlap)
4. **Generate embeddings** (OpenAI text-embedding-3-small)
5. **Upload to Pinecone** baseline namespace with metadata

### Expected Output

```
ðŸ“š INDEXING BASELINE CORPUS
============================================================
Total documents: 92
Target namespace: baseline
Dry run: False
============================================================

[1/92] google_tos.pdf
ðŸ“„ Processing: Google
   1/5 Extracting text...
   2/5 Parsing structure...
       Found 47 clauses
   3/5 Creating chunks...
       Created 23 chunks
   4/5 Generating embeddings...
   5/5 Uploading to Pinecone baseline namespace...
âœ“ Indexed: Google (23 chunks, 8.3s)
Progress: 1.1% (1/92)

...

ðŸ“Š INDEXING SUMMARY
============================================================
âœ“ Successful:     89
âŠ™ Skipped:        3
âœ— Failed:         0
ðŸ“„ Total Docs:    92
ðŸ“¦ Total Chunks:  2,145
ðŸ“ Total Chars:   3,234,567
â±ï¸  Avg Time/Doc:  7.2s
ðŸ“Š Avg Chunks/Doc: 24.1

ðŸ’° Estimated OpenAI Cost: $0.04

âœ… Indexing complete!
```

### Performance & Costs

| Metric | Expected Value |
|--------|----------------|
| **Processing Time** | 2-3 minutes per document |
| **Total Time (100 docs)** | 3-5 hours |
| **OpenAI Embedding Cost** | ~$0.04 per 100 documents |
| **Pinecone Storage** | ~10MB per 1000 vectors |

### Monitoring Indexing Progress

```bash
# Check Pinecone index stats
python scripts/analyze_corpus_stats.py --check-index

# View logs
tail -f backend/corpus_indexing.log
```

---

## ðŸ“Š Statistics & Analysis

### Generate Statistics

```bash
# Basic statistics
python scripts/analyze_corpus_stats.py

# Include Pinecone index stats
python scripts/analyze_corpus_stats.py --check-index

# Generate visualizations
python scripts/analyze_corpus_stats.py --visualize

# Save detailed report
python scripts/analyze_corpus_stats.py --output corpus_stats.json
```

### Statistics Included

1. **Document Statistics**
   - Total documents, pages, characters
   - Average and median values
   - Page count distribution

2. **Category Distribution**
   - Documents per category
   - Balance scores
   - Average content per category

3. **Pinecone Index**
   - Total vectors
   - Vectors per namespace
   - Average vectors per document

4. **Quality Metrics**
   - Completeness score (vs 100 doc target)
   - Diversity score (category balance)
   - Length quality score
   - Overall quality score

### Example Output

```
ðŸ“Š BASELINE CORPUS STATISTICS
======================================================================

ðŸ“„ DOCUMENT STATISTICS:
   Total Documents:      92
   Total Pages:          852
   Total Characters:     3,456,789
   Average Pages/Doc:    9.3
   Average Chars/Doc:    37,573
   Page Range:           2 - 45
   Median Pages:         8

   Extraction Methods:
      pdfplumber     :  87 (94.6%)
      pypdf2         :   5 (5.4%)

ðŸ“ CATEGORY DISTRIBUTION:
   tech           :  25 docs, 10.2 avg pages, balance: 0.98
   ecommerce      :  23 docs,  8.7 avg pages, balance: 0.95
   saas           :  20 docs,  9.1 avg pages, balance: 0.89
   finance        :  15 docs,  8.3 avg pages, balance: 0.78
   general        :   9 docs,  9.8 avg pages, balance: 0.65

ðŸ” PINECONE INDEX:
   Index Name:           tc-analysis
   Total Vectors:        2,198
   Dimension:            1536
   Namespaces:
      baseline         : 2,145 vectors
      user_tcs         :    53 vectors
   Avg Vectors/Doc:      23.3

âœ… QUALITY METRICS:
   Completeness:         92.0%
   Diversity:            85.0%
   Length Quality:       95.0%
   Overall Score:        90.3%

ðŸ’¡ RECOMMENDATIONS:
   â€¢ Add 8 more documents to reach target of 100
   â€¢ Category 'general' has only 9 documents (target: 20+)
   â€¢ Category 'finance' has only 15 documents (target: 20+)

======================================================================
```

---

## ðŸŽ¯ Quality Guidelines

### Document Requirements

âœ… **Must Have:**
- Minimum 2 pages
- Minimum 500 characters
- Clean text extraction
- Recent version (within 2 years)
- English language

âš ï¸ **Avoid:**
- Scanned PDFs without OCR
- Non-English documents
- Privacy policies (we want T&Cs)
- Cookie policies
- Marketing materials

### Category Balance

Target distribution:
```
tech       : 25 documents (26%)
ecommerce  : 25 documents (26%)
saas       : 20 documents (21%)
finance    : 15 documents (16%)
general    : 25 documents (26%)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Total      : 110 documents
```

### Diversity Within Categories

**Tech**: Mix of social media, cloud services, developer tools
**E-commerce**: Marketplaces, retailers, fashion brands
**SaaS**: Productivity, collaboration, CRM, communication
**Finance**: Payments, banking, crypto, fintech
**General**: Services, entertainment, travel, food delivery

---

## ðŸ”§ Advanced Configuration

### Collection Script Configuration

Edit `scripts/collect_baseline_corpus.py`:

```python
# Add new sources to BASELINE_SOURCES dict
BASELINE_SOURCES = {
    "tech": [
        {"name": "NewCompany", "url": "https://...", "type": "web"},
        # ... more sources
    ],
}

# Adjust timeouts
await page.goto(url, wait_until="networkidle", timeout=60000)  # 60s

# Adjust PDF settings
await page.pdf(
    path=str(output_path),
    format="A4",
    print_background=True,
    margin={"top": "1cm", "right": "1cm", "bottom": "1cm", "left": "1cm"}
)
```

### Indexing Script Configuration

Edit `scripts/index_baseline_corpus.py` or `backend/app/core/legal_chunker.py`:

```python
# Adjust chunk size
chunker = LegalChunker(
    max_chunk_size=500,  # Increase for longer chunks
    overlap=50           # Increase for more context
)

# Adjust batch delay
await asyncio.sleep(1.0)  # Seconds between documents
```

---

## ðŸ› Troubleshooting

### Common Issues

#### 1. Playwright Installation Fails

**Error**: `playwright executable not found`

**Solution**:
```bash
pip install playwright --upgrade
playwright install chromium
```

#### 2. Indexing Takes Too Long

**Cause**: Processing 100+ documents with embeddings is slow

**Solutions**:
- Run overnight
- Process categories separately
- Use `--dry-run` first to test

#### 3. Pinecone Quota Exceeded

**Error**: `429 Too Many Requests`

**Solution**:
- Check Pinecone quotas
- Increase batch delay: `--delay 2.0`
- Upgrade Pinecone plan if needed

#### 4. OpenAI Rate Limit

**Error**: `Rate limit exceeded`

**Solution**:
- Batch operations are already used
- Wait and retry
- Check API tier limits

#### 5. Duplicate Documents

**Issue**: Validator reports duplicates

**Solution**:
- Review both documents
- Keep if different versions/jurisdictions
- Remove if truly duplicate

---

## ðŸ“š Data Sources

### Pre-configured Sources (95 companies)

The collection script includes URLs for:

- **25 Tech companies**: Google, Facebook, Microsoft, Apple, etc.
- **25 E-commerce**: Amazon, eBay, Etsy, Walmart, etc.
- **20 SaaS**: Slack, Notion, Asana, Salesforce, etc.
- **15 Finance**: PayPal, Stripe, Coinbase, Robinhood, etc.
- **25 General**: Uber, Airbnb, Netflix, Spotify, etc.

### Adding New Sources

Edit `scripts/collect_baseline_corpus.py`:

```python
BASELINE_SOURCES = {
    "your_category": [
        {
            "name": "CompanyName",
            "url": "https://company.com/terms",
            "type": "web"  # or "pdf" for direct PDF links
        },
    ]
}
```

---

## ðŸš€ Next Steps

After completing corpus collection:

1. âœ… **Validate corpus** (should have 90%+ valid files)
2. âœ… **Index to Pinecone** baseline namespace
3. âœ… **Analyze statistics** (check quality score)
4. âœ… **Test anomaly detection** with user documents
5. âž¡ï¸ **Week 8-10**: Build frontend interface

---

## ðŸ“ž Support

### Issues & Questions

- Check troubleshooting section
- Review log files:
  - `corpus_collection.log`
  - `corpus_validation.log`
  - `corpus_indexing.log`
  - `corpus_analysis.log`

### Manual Review Needed

Some high-quality sources may require manual collection:
- Complex JavaScript-heavy sites
- Sites with captcha/login requirements
- Government/legal institution T&Cs
- Industry-specific terms (healthcare, finance, etc.)

---

## ðŸ“ˆ Success Criteria

Your corpus is ready when:

- âœ… **90+ documents** collected
- âœ… **5 categories** represented
- âœ… **Validation passes** (90%+ valid)
- âœ… **Indexed to Pinecone** baseline namespace
- âœ… **Quality score > 85%**
- âœ… **No critical errors** in validation
- âœ… **Category balance** reasonable (15+ docs each)

---

## ðŸŽ‰ Completion Checklist

- [ ] Scripts installed and working
- [ ] Automated collection run successfully
- [ ] Manual collection completed for failed sources
- [ ] Validation passed with 90%+ valid
- [ ] All documents indexed to Pinecone
- [ ] Statistics analyzed and quality score good
- [ ] Metadata complete
- [ ] Ready for anomaly detection testing!

---

**Next**: [Testing Anomaly Detection](TESTING_ANOMALY_DETECTION.md)
